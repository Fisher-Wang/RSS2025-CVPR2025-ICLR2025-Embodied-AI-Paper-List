# CVPR2025-ICLR2025-Embodied-AI-Paper-List
ðŸ”¥CVPR2025 &amp; ICLR2025 Embodied AI Paper List  Resources.

## ðŸ“– Paper List
- [ðŸ“– CVPR2025](#cvpr2025)
  - [Vision-Language-Action Models](#vision-language-action-models)
  - [Policies](#policies)
  - [Grasp](#grasp)
  - [Humanoid](#humanoid)
  - [Planning and Reasoning](#planning-and-reasoning)
  - [3D Vision](#3d-vision)
  - [Sim2real and Real2sim](#sim2real-and-real2sim)
  - [Benchmark and Dataset](#benchmark-and-dataset)
- [ðŸ“– ICLR2025](#iclr2025)
  - [Vision-Language-Action Models](#vision-language-action-models)
  - [Policies](#policies)
  - [Planning and Reasoning](#planning-and-reasoning)
  - [3D Vision](#3d-vision)
  - [Sim2real and Real2sim](#sim2real-and-real2sim)
- [ðŸ“– ICRA2024](#icra2024)
  
# CVPR2025

## Vision-Language-Action Models
- **MoManipVLA**: Transferring Vision-language-action Models for General Mobile Manipulation [Paper](https://arxiv.org/abs/2503.13446)
- **CoT-VLA**: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models [Paper](https://cvpr.thecvf.com/virtual/2025/poster/33233)
- **SOLAMI**: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters [Paper](https://arxiv.org/abs/2412.00174)
- A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning [Paper](https://arxiv.org/abs/2503.06960) [Page](https://github.com/CVMI-Lab/SlotMIM)
- **Think Small, Act Big**: Primitive Prompt Learning for Lifelong Robot Manipulation
- **Phoenix**: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction [Paper](https://cvpr.thecvf.com/virtual/2025/poster/32789)
- **OmniManip**: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints [Paper](https://arxiv.org/abs/2501.03841)
- Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation [Paper](https://arxiv.org/abs/2406.14235)
- Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation [Paper](https://cvpr.thecvf.com/virtual/2025/poster/34522)
- Robotic Visual Instruction
- **RoboGround**: Robot Manipulation with Grounded Vision-Language Priors

## Policies
- **KStar Diffuser**: Spatial-Temporal Graph Diffusion Policy with Kinematics Modeling for Bimanual Robotic Manipulation [Paper](https://arxiv.org/abs/2503.10743)
- **RoboPEPP**: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training [Paper](https://arxiv.org/abs/2411.17662)
- **Lift3D Policy**: Lifting 2D Foundation Models for Robust 3D Robotic Manipulation [Paper](https://arxiv.org/abs/2411.18623) [Page](https://lift3d-web.github.io/)
- **PDFactor**: Learning Tri-Perspective View Policy Diffusion Field for Multi-Task Robotic Manipulation [Paper](https://cvpr.thecvf.com/virtual/2025/poster/33943)
- **Two by Two**: Learning Cross-Task Pairwise Objects Assembly for Generalizable Robot Manipulation
- **FlowRAM**: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation [Paper](https://cvpr.thecvf.com/virtual/2025/poster/33579)
- **G3Flow**: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation [Paper](https://arxiv.org/abs/2411.18369) [Page](https://tianxingchen.github.io/G3Flow/)
- **DexHandDiff**: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation [Paper](https://arxiv.org/abs/2411.18562)
- **Tra-MoE**: Learning Trajectory Prediction Model from Multiple Domains for Adaptive Policy Conditioning [Paper](https://arxiv.org/abs/2411.14519)
- **AffordDP**: Generalizable Diffusion Policy with Transferable Affordance[Paper](https://arxiv.org/abs/2412.03142)

## Grasp
- **UniGraspTransformer**: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping [Paper](https://arxiv.org/abs/2412.02699) [Page](https://dexhand.github.io/UniGraspTransformer/)
- **DexGrasp Anything**: Towards Universal Robotic Dexterous Grasping with Physics Awareness [Paper](https://arxiv.org/abs/2503.08257)
- **ZeroGrasp**: Zero-Shot Shape Reconstruction Enabled Robotic Grasping [Paper](https://cvpr.thecvf.com/virtual/2025/poster/32440)

## Humanoid
- Let Humanoid Robots Go Hiking! Integrative Skill Development over Complex Trails [Page](https://lego-h-humanoidrobothiking.github.io/)
- **MobileH2R**: Learning Generalizable Human to Mobile Robot Handover Exclusively from Scalable and Diverse Synthetic Data [Paper](https://arxiv.org/abs/2501.04595)

## 3D Vision
- **3D-MVP**: 3D Multiview Pretraining for Robotic Manipulation [Paper](https://arxiv.org/abs/2406.18158)
- **VidBot**: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation[Paper](https://arxiv.org/abs/2503.07135)

## Planning and Reasoning
- **RoboBrain**: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete [Paper](https://arxiv.org/abs/2502.21257)
- **PhysVLM**: Enabling Visual Language Models to Understand Robotic Physical Reachability [Paper](https://arxiv.org/abs/2503.08481)
- **RoboSpatial**: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics [Paper](https://arxiv.org/abs/2411.16537)
- **Tartan IMU**: A Light Foundation Model for Inertial Positioning in Robotics [Paper](https://cvpr.thecvf.com/virtual/2025/poster/33873)
- **Code-as-Monitor**: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection [Paper](https://arxiv.org/abs/2412.04455) [Page](https://zhoues.github.io/Code-as-Monitor/)

## Video
- **Roger**: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation
- **GraphMimic**: Graph-to-Graphs Generative Modeling from Videos for Policy Learning [abs](https://cvpr.thecvf.com/virtual/2025/poster/34942)

## Sim2real and Real2sim
- **Prof. Robot**: Differentiable Robot Rendering Without Static and Self-Collisions [Paper](https://arxiv.org/abs/2503.11269)
- **AutoURDF**: Unsupervised Robot Modeling from Point Cloud Frames Using Cluster Registration [Paper](https://arxiv.org/abs/2412.05507) [Page](https://github.com/jl6017/AutoURDF)

## Benchmark and Dataset
- **RoboTwin**: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)[Paper](https://arxiv.org/abs/2409.02920) [Page](https://robotwin-benchmark.github.io/early-version/)
- Pixel-aligned RGB-NIR Stereo Imaging and Dataset for Robot Vision [Paper](https://arxiv.org/abs/2411.18025)
- **RoboSense**: Large-scale Dataset and Benchmark for Egocentric Robot Perception and Navigation in Crowded and Unstructured Environments [Paper](https://arxiv.org/abs/2408.15503)

# ICLR2025

## Vision-Language-Action Models
- **VLAS**: Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation [Paper](https://arxiv.org/abs/2502.13508)
- **TraceVLA**: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies [Paper](https://arxiv.org/abs/2412.10345) [Page](https://tracevla.github.io/)
- **Robots Pre-train Robots**: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets [Paper](https://arxiv.org/abs/2410.22325) [Page](https://robots-pretrain-robots.github.io/)
- **PIDM**:Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation [Paper](https://arxiv.org/abs/2412.15109)

## Policies
- **ReViWo**: Learning View-invariant World Models for Visual Robotic Manipulation [zhihu](https://zhuanlan.zhihu.com/p/26181243574)
- **HAMSTER**: Hierarchical Action Models For Open-World Robot Manipulation [Paper](https://arxiv.org/abs/2502.05485)
- **BadRobot**: Jailbreaking Embodied LLMs in the Physical World [Paper](https://arxiv.org/abs/2407.20242)
- **STRAP**: Robot Sub-Trajectory Retrieval for Augmented Policy Learning [Paper](https://arxiv.org/abs/2412.15182) [Project](https://weirdlabuw.github.io/strap/)
- **SRSA**: Skill Retrieval and Adaptation for Robotic Assembly Tasks [Paper](https://arxiv.org/abs/2503.04538)
- **LLaRA**: Supercharging Robot Learning Data for Vision-Language Policy [Paper] (https://arxiv.org/abs/2406.20095)
- Data Scaling Laws in Imitation Learning for Robotic Manipulation [Paper] (https://arxiv.org/abs/2410.18647)
- **Stem-OB**: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion [Paper] (https://arxiv.org/abs/2411.04919)

## 3D Vision
- **Dream to Manipulate**: Compositional World Models Empowering Robot Imitation Learning with Imagination [Paper](https://arxiv.org/abs/2412.14957)[Page](https://leobarcellona.github.io/DreamToManipulate/)

## Planning and Reasoning
- **LASeR**: Towards Diversified and Generalizable Robot Design with Large Language Models [Paper](https://openreview.net/forum?id=7mlvOHL6qJ)
- Physics-informed Temporal Difference Metric Learning for Robot Motion Planning [Paper](https://openreview.net/forum?id=TOiageVNru)
- **AHA**: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation [Paper](https://arxiv.org/abs/2410.00371) [Page](https://aha-vlm.github.io/)
- **EMOS**: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents[Paper](https://arxiv.org/abs/2410.22662) [Page](https://arxiv.org/abs/2410.22662)
- **VisualPredicator**: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning [Page](https://arxiv.org/abs/2410.23156)
- DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from a Single Demo [Paper](https://arxiv.org/abs/2412.05268)
- 6D Object Pose Tracking in Internet Videos for Robotic Manipulation [Paper](https://arxiv.org/abs/2503.10307)

## Planning and Reasoning
- Multi-Robot Motion Planning with Diffusion Models [Paper](https://arxiv.org/abs/2410.03072)

## Video
- **GEVRM**: Goal-Expressive Video Generation Model For Robust Visual Manipulation [Paper](https://arxiv.org/abs/2502.09268)

## Sim2real and Real2sim
- **ReGen**: Generative Robot Simulation via Inverse Design [Paper](https://openreview.net/forum?id=EbCUbPZjM1)
  
# ICRA2024
- **MoRE**: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models [Paper](https://arxiv.org/abs/2503.08007)
- **QUART-Online**: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning [Paper](https://arxiv.org/abs/2412.15576)

